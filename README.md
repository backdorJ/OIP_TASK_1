# Краулер для выкачки текстовых страниц (ОИП, задание 1)
# Набиуллин Дамир Рафаэлевич 11-208

Скачивает не менее 100 текстовых страниц из предварительно подготовленного списка URL, сохраняет каждую в отдельный текстовый файл (вместе с HTML-разметкой) и формирует `index.txt` (номер файла и ссылка).

## Требования

- Python 3.8+ (достаточно стандартной библиотеки, установка пакетов не обязательна)

## Подготовка списка URL

Запустите данный скрипт, чтобы сгенерировать файл `urls.txt` где будут находится ссылки для выкачки

```bash
python get_url_list.py
```

## Запуск краулера

```bash
python crawler.py
```

Краулер:

1. Читает URL из `urls.txt`.
2. Выкачивает страницы
3. Сохраняет каждую страницу в папку `pages/` как `1.txt`, `2.txt`, ...
4. Создаёт в корне проекта файл `index.txt` в формате: `номер_файла<TAB>URL`.

## Результат

- **pages/** — выкачанные страницы (`1.txt`, `2.txt`, …).
- **index.txt** — соответствие номера файла и ссылки.
